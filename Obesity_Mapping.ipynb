{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied GIS Practical: Obesity Mapping with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to a jupyter notebook! A jupyter notebook is what looks like to be a google word document but just with lots of code and other things in it... but it is much more.\n",
    "\n",
    "Within a jupyter notebook, you can write and execute code just like you might if using the command line or a provided script editor. But you can also add text and format it so you can create things like this - a step by step guide to something... It's become a very popular tool within research communities as you can share code and try out ideas easily, just like we're doing here. It was primarly used for the python programming language (and used to be called an ipython notebook) but now can be used for many other languages. \n",
    "\n",
    "In the code blocks below, you have access to all the code necessary to quickly process the data you'll need to map the obesity data. There will be general explanations to what we're doing and how the code feeds into this. You'll also be prompted where you need to change inputs, such as file paths to the right data. This will **not** teach you how to code but intends to provide you with an overview of how you can use programming to automate your work. You can use the general learnings here to decide if and how you might want to move forward with programming in your future GIS career. For example, the Python language used here - and many of the fundamental methods: for loops, conditional statements - can be used within ArcPy, a python-based console built within ArcGIS. We will not use ArcPy in this assignment, but there are plenty of online tutorials to help you use it. It's a great tool to try out - even if you simply learn how to bring in data and map it using the ArcPy interface rather than just by 'click and point', you'll appreciate how quickly you can repeat processing and coplete some more mundane tasks! \n",
    "\n",
    "This practical is **a lot** of reading and the majority of the code is already written for you - but you will be editing and executing code to complete your task, so you will need to pay attention to the nitty gritty details. If you take your programming education further, you'll be able to use this notebook in future analyses as many of the methods and functions used below are things you're likely to use again.\n",
    "\n",
    "But first... \n",
    "\n",
    "---\n",
    "\n",
    "## Part One: Jupyter Notebook Basics\n",
    "\n",
    "\n",
    "1) This notebook is made up of two types of cells: **Markdown** (or text, such as the instructions you are reading right now!) and **Code**. \n",
    "\n",
    "  * To access a Markdown cell, you need to double click into the cell. To execute a Markdown cell (i.e. format it as text), you need to **hold SHIFT and press RETURN**. (Try it now and see what happens! Double-click to edit this text box and then run your new cell).\n",
    "  \n",
    "  \n",
    "  * To access a Code cell, you just need a single click. To execute a Cell code (**i.e. run the code in that cell block**), you also need to **hold SHIFT and press RETURN**. \n",
    "  \n",
    "  ##### FOR THIS TUTORIAL TO WORK, YOU NEED TO EXECUTE EACH BLOCK OF CODE. THE CODE IS THE TEXT IN THE GREYED OUT BOXES AND IN MANY CASES WILL PRODUCE AN OUTPUT THAT IS PRINTED BELOW THE GREY BOX. WE WILL EXECUTE THE CODE IN STEPS.\n",
    "  \n",
    "  \n",
    "  * The code is executed by the kernel. A notebook kernel is a “computational engine” that executes the code contained in a Notebook document. \n",
    "        \n",
    "2) Running the code in notebooks is a step by step process - but they all feed into one another. For example, if you try to run the last bit of code at the bottom of this notebook, it will fail. This is because we need to run all other code blocks first. As we are working through this notebook step-by-step, this should not be an issue. **BUT if you do make a mistake or error, I would recommend stopping the 'Kernel' (i.e. the background processing and memory)**. To do this, click 'Kernel' on the above menu and select **'Restart and Clear Output'**. Then go through and run each block of code again. There are quick ways to do this (e.g. Click Cell on the menu bar and see options), but I'd only recommend doing this when you feel comfortable with using the notebook.\n",
    "\n",
    "3) If you haven't written code before, don't worry if this feels overwhelming. It's meant to - think about back to your second year first GIS module and how it felt in those first few classes. It's unfamiliar and not the most intuitive thing to do - but keep going with it. You've got everything you need to complete the practical here - just start again or ask for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - A quick introduction to Python and its format -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated, we're using Python for our analysis. Python is an interpreted programming language, which allows us to quickly run code and get outputs without having to do much else. All programming languages have a standard library - or in other words, things it knows how to do straight away without any add-ons. This usually involves basic maths, functions such as printing to the screen, and techniques such as for loops and conditional statements.\n",
    "\n",
    "Let's have a look at the first few code blocks below - and execute them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we go....\n",
    "# The hashtag here allows us to provide comments to our code i.e. annotate it!\n",
    "\n",
    "# Basic maths\n",
    "3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a variable\n",
    "# A variable is a value that is stored with a name - the value is the result of 3 + 3; basic_maths is the variable name.\n",
    "basic_maths = 3+3\n",
    "\n",
    "# Print a variable\n",
    "# Print is a function i.e. it does something to something else\n",
    "# That something else is passed through in the brackets\n",
    "print(basic_maths)\n",
    "\n",
    "# Redefine our variable using a new list of data\n",
    "# A list is signified using square brackets,[], and separating values in our list by commas.\n",
    "basic_maths = [3+3, 6*4, 5-0, 45/5]\n",
    "\n",
    "# Print our new variable - notice it is the results of our basic maths\n",
    "print(basic_maths)\n",
    "\n",
    "# Creating a for loop - can you figure out what is happening?\n",
    "for number in basic_maths:\n",
    "    print (\"Yes, this is my number: \" + str(number))\n",
    "\n",
    "# For every number in the variable basic_maths i.e. our list of FOUR numbers, print the statement.\n",
    "# The statement is made of a string, indicated by the \"\", a + sign which adds the next part of the statement\n",
    "# Which is our number, converted from an integer to a string 'on the fly' by the code.\n",
    "# Str() is a function that does this conversion\n",
    "# How many times has the statement printed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a conditional if statement within a for loop. \n",
    "\n",
    "# Note '%' asks whether a remainder is left when dividing the first number by the second number.\n",
    "# In this case, we are asking what happens if the number can be divided by 2.\n",
    "# If it leaves a remainder, the statement is TRUE, we return the first print string...\n",
    "# ...that the number is odd\n",
    "# If it does not leave a remainder, the statement is FALSE, we return the second (else) print strng...\n",
    "# ...that the number is even\n",
    "\n",
    "for number in basic_maths:\n",
    "    if number % 2:\n",
    "        # The if statement returns a True or False value\n",
    "        # Here if it True that the number leaves a remainder, then we know the number is odd.\n",
    "        print (\"yes, we have a number! And it is odd.\")\n",
    "    else:\n",
    "        # If the number does not leave a remainder and thus returns False, we know it is even.\n",
    "        print (\"yes, we have a number! And it is even.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our for loop works because it has the variable defined in the code block above. If you want, try restarting the Kernel and see what happens when you run just this code (Block 19) on its own, without the previous code block? \n",
    "\n",
    "*You should see the error: name 'basic_maths' is not defined!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Notation: Indenting, ( ) and . !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indenting in python is more than just trying to make readable code - which is quite different to other programming languages, where indenting is just to make the code look pretty.  Indentation is required for indicating what block of code a statement belongs to and thus provides the logic with which a computer will process code.\n",
    "\n",
    "You'll have seen in the code above that we use () (parentheses) to pass a variable through something we're calling a function i.e. it does something to something else. In the parentheses, you'll find the arguments that the function uses e.g. a variable, or some other things you might need in the function, a few examples of these are used below. \n",
    "\n",
    "Also in the code below, you'll also see the dot notation, used on its own and with parentheses. The dot notation is associated with the basic Object-Orientated Programming for which python is commonly used for. The dots and parentheses are used with classes, objects, instances and methods. These are some of the fundamental ideas behind using python but are, for many, the most complex - or at least time-consuming - to understand and apply. For this tutorial, it's beyond the scope of what we're tryign to teach. If you'd like to read more about it, have a look at this post: http://reeborg.ca/docs/oop_py_en/oop.html . But don't fear, you won't need to know the distinctions for the rest of the tutorial, but if you do take programming further, it will be something you come across time and time again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Python Packages/Libraries - extending functionality **\n",
    "\n",
    "In addition to the main library of the Python language, additional packages (or libraries) have been created by users (people just like you and me!) that provide additional functionality to allow python users to do all sorts of tasks and also specialise in specific areas.\n",
    "\n",
    "Python was first released as a programming language in 1991, but the support now available for **geographical analysis** has only really developed over the last fifteen or so years. Luckily for you early career GISers, there are plenty of libraries that we can use to do all sorts of things, from statistical analysis, GIS, to creating web maps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part Two: Processing a single CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to conduct any analysis, we first need to process our data. We need to assign tertiles to our data and then join this to the LAD shapefile. The below code will walk you through exactly how to do this, without excel or ArcGIS.\n",
    "\n",
    "If you had read through the `Practical Instructions` file, you should by now have a complete set of data for obesity for each year range: \n",
    "* 2011-2012\n",
    "* 2012-2013\n",
    "* 2013-2014\n",
    "* 2014-2015\n",
    "* 2015-2016\n",
    "* 2016-2017 *(you should have downloaded this file and cleaned it as per the instructions)*\n",
    "\n",
    "For this analysis to work, we need each table to have the same format as provided and saved in it's own individual csv.\n",
    "\n",
    "In addition, you should have created a new folder called `Data` within the folder that contains this notebook. Within this folder, you should have created another folder called `Obesity`. You should then have moved each processed CSV into this folder. Each CSV will be named in the same convention - here we are using the first year of the year range to denote the year of the CSV i.e. 2011-2012 becomes 2011. All raw data (i.e. the original spreadsheet workbooks downloaded from NHS digital) should be in the `RAW` folder.\n",
    "\n",
    "If you are at this stage, you can now proceed with the notebook. If you're stuck or unsure if you've completed these steps, please ask for help now rather than continuing with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Setting up our notebook -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis, we want to import several libraries that will help us. The libraries are already installed on the computer, we just now need to tell the notebook that (a) we want to use them, using the import tool and (b) how we will refer to them in our code, using the ```as``` function. As you can see, it's useful to create shorthand names for the libraries - the ones used below are standard convention for the libraries we will use.\n",
    "\n",
    "**NOTE**: To import geopandas and imageio (see later), we need to install these first from the Anaconda prompt command line. To do so (if you are not in the practical and thus not there for the walkthrough):\n",
    "\n",
    "* Open up the anaconda prompt program FROM the Anaconda 3 folder. Go to All Programs --> Programming --> Anaconda 3 --> Anaconda Prompt.\n",
    "\n",
    "* It appears as an empty black box, you need to wait for it to display some text (which denotes the program and current directory) followed by an >\n",
    "\n",
    "* After the ```>```\n",
    "\n",
    "* Type: ```conda install -c conda-forge geopandas```\n",
    "\n",
    "* You'll see the word 'Fetching', followed by lots of lines of installation code.\n",
    "\n",
    "* Type ```y``` to override/supersede if asked\n",
    "\n",
    "* If your computer displays ONLY a blinking line on the bottom line, your computer is still installing the software - patience!\n",
    "\n",
    "* Wait for the prompt to finish installation - your display will now show your pathway again  followed by the blinking line ``` _ ```.\n",
    "\n",
    "* Once finished, install the imageio library\n",
    "\n",
    "* Type: ```conda install -c conda-forge imageio```\n",
    "\n",
    "* Again type ```y``` to override/supersede if asked\n",
    "\n",
    "* It will be much quicker than the previous library as it only has 4 packages to install.\n",
    "\n",
    "* Once your path appears again with a blinking line, you need to refresh this notebook by pressing ```Ctrl+R```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the appropriate modules/libraries\n",
    "\n",
    "# Pandas = used for data processing and analysis.\n",
    "import pandas as pd\n",
    "\n",
    "# GeoPandas = used for geospatial data processing and mapping (allows you to make maps!).\n",
    "import geopandas as gpd\n",
    "\n",
    "# NumPy = used for high level mathematics and statistical analysis\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib = used to plot analysis usually resulting from using the numpy library\n",
    "import matplotlib as plt\n",
    "\n",
    "# In addition, although not used here: SciPy - common library for scientific and engineering processing; Pysal - support for specific geospatial data analysis \n",
    "# And so many more! For now, we'll stick with these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Reading in our data - ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to set the pathway to our data, so we can read in each file for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data path can be as simple as:\n",
    "data_path = \"Data/Obesity/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then set each of files to equal a variable that we'll use in our analysis. Let's start with the 2016 file that you should have cleaned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob16 = data_path + '2016_LA_OBESITY.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, our variable only stores the path to the file. We now need to actually read in the CSV so the data at the path gets stored in our variable and we can do some data processing and analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the read_csv function from the pandas library to read in the csv\n",
    "# Denoted as pd.read_csv(FILETOREAD/VARIABLE). The ' pd ' = use the pandas library!\n",
    "ob16_csv = pd.read_csv(ob16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check to see if our data has been read in correctly. We'll look at the data structure and the overall contents of the data. First we used the .head() function to look at the top five lines of our CSV, which helps us understand the structure of our table and the field names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head function/method\n",
    "ob16_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at the overall contents of the file by looking at the info file. This is almost like looking at the metadata of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info function/method\n",
    "ob16_csv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above that our table has been loaded in as a Pandas data frame - which is exactly what we want in order to use the pandas library for our further processing.\n",
    "\n",
    "We can also see that we have three columns and check that the file names have been imported correctly.\n",
    "\n",
    "We're also looking for 3 columns and 326 rows (326 LAs and 1 header), which it looks like again we've got. We also don't want to have any null (i.e. empty cells) so we check that we have 326 non-null entries for each column (non-null obviously meaning not empty!). \n",
    "\n",
    "Finally we can tell what data types we have in our spreadsheet. Our LA Names and Codes have been interpreted as 'Objects' and our Obesity Score has been interpreted as a float. If this is not the case in your data, let us know now as you won't be able to proceed with the tutorial without the data in this format!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Creating the tertile data - ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know we've got our data loaded correctly and it's ready to be worked on.... so let's get to it. We need to first assign our tertiles, and luckily NumPy has a function that can help us do just that. Below is our big bit of code that will determine our breakpoints and then assign to each LA a tertile depending on where it's obesity score falls within these breakpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Identify breakpoints within OBESEPERYR6 using the percentile method from numpy library. #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentile function is denoted as: np.percentile(VARIABLE, BREAKPOINT) \n",
    "\n",
    "As you can see, our percentile function here takes two arguments:\n",
    "1) VARIABLE: the variable we wish to create the percentiles with. Note, we access the 'OBESEPERYR6' values by using the ob16_csv variable and selecting [or indexing] the 'OBESEPERYR6' column.\n",
    "2) BREAKPOINT: the percentile breakpoint - you determine what breakpoint you'd like e.g. 3.33 (%) = 1st tertile, 66.66 (%)= 2nd tertile. Another example: you would use 20/40/60/80 to create quintiles.\n",
    "\n",
    "The code below will then assign the numerical value of each breakpoint to the variables: tertile1 and tertile2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tertile1 = np.percentile(ob16_csv['OBESEPERYR6'], 33.33)\n",
    "tertile2 = np.percentile(ob16_csv['OBESEPERYR6'], 66.66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For your info:*\n",
    "\n",
    "The square brackets [ ] used are a form of indexing, which again is beyond the scope for this tutorial but is something fundamental to python. Any introduction to python tutorial will teach you about indexing, particularly when dealing with lists. In our case, we use the [ ] to index the rows and columns in the csv. So CSV[COLUMN] returns the values of the column you've indexed within the CSV. You don't need to know any more detail than this for the tutorial, but make a note, as it's something you'll come across in future programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Check percentile function has worked #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that our code has worked, we will print the tertile breakpoints to the screen - and just check that they make sense. In our code below, the ```str()``` function here is used so that the breakpoints, which will be a float data type, can be printed - essentially we convert the float to a string, which allows it to be printed with another string. Alternatively, we could just print the float on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this code to print the breakpoints - and check that our code has worked:\n",
    "print ('Tertile 1 = '+ str(tertile1))\n",
    "print('Tertile 2 = '+ str(tertile2))\n",
    "\n",
    "print (tertile1)\n",
    "print (tertile2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Create Tertile column for filling with tertile value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create simply an empty column with the field name Tertile, ready to be filled with the appropriate tertile value. To do so, we create a new Tertile column to the ob16_csv variable and assign it the value of 0. By doing so adds the column to our dataset - but this is only within the computer memory (i.e. the processing of this python notebook). To add the column permanently, we will need to export the dataset to a CSV, which we will do later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob16_csv['Tertile'] = 0\n",
    "\n",
    "# Have a look by executing this code!\n",
    "ob16_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Assign each LA a tertile value based on OBESEPERYR6 score and breakpoints using a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign each LA a tertile value with relative ease, we're going to use several 'tools' of python: a function, a for loop and conditional statements. \n",
    "\n",
    "We're going to create our own function by writing a small script that details what we want the computer to do. We can then reuse this function again and again i.e. for our other CSVs. As stated earlier, a function is something that will do something to something else, and we're going to use our for loop to provide most of this muscle and the conditional statement to tell the for loop what to do. It might sound a little confusing but hopefully you'll see how it all works out in the code - remember we're creating a functin that determines the value of Tertile column for each LA by comparing it to our tertile breakpoints that we've already defined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the function \" create_tertile \":\n",
    "\n",
    "def create_tertile():\n",
    "    \n",
    "    # Next we check that everything we want to be in our function gets indented within the function.\n",
    "    # Then we create documentation about what the function is, just common practice!\n",
    "    \"\"\"\n",
    "    Assigns each LA a tertile based on their obesity value against the defined breakpoints    \n",
    "    -------\n",
    "    \"\"\"\n",
    "    # As you can see, we've duplicated our tertile code from above: \n",
    "    tertile1 = np.percentile(ob16_csv['OBESEPERYR6'], 33.33)\n",
    "    tertile2 = np.percentile(ob16_csv['OBESEPERYR6'], 66.66)\n",
    "    # It is good practice that a function contains the variables it's using. \n",
    "    # I.e so it has no missing links that could get break the code.\n",
    "\n",
    "    # We'll now add in a FOR LOOP, which will allow us to apply a conditional test to each LA row\n",
    "    # And then assign a value based on the outcome of this test!\n",
    "\n",
    "    for i in range(len(ob16_csv['OBESEPERYR6'])):\n",
    "        # We won't go into the detail here, but essentially what the above FOR statement does is:\n",
    "        # For each observation (i.e. LA) in the range 0-no.of observations in the column OBESEPERYR6\n",
    "        # in the ob16_csv (i.e. 326 observations)... or in other words...\n",
    "        \n",
    "        # For each of the 326 LAs:\n",
    "        # (Again, note the indentation!)        \n",
    "        # If the value found at the intersection of LA row and OBESEPERYR6 is less than or equal to tertile1\n",
    "        # Assign a value of 1 to the Tertile column\n",
    "           if ob16_csv.loc[i,'OBESEPERYR6'] <= tertile1:\n",
    "                ob16_csv.loc[i,'Tertile'] = 1\n",
    "            # Else-if the value is more than tertile1 but less than or equal to tertile2\n",
    "            # Assign a value of 2 to the Tertile column\n",
    "           elif ob16_csv.loc[i,'OBESEPERYR6'] > tertile1 and ob16_csv.loc[i,'OBESEPERYR6'] <= tertile2:\n",
    "                ob16_csv.loc[i,'Tertile'] = 2\n",
    "            # Else, in all other cases, assign a value of 3 to the tertile column.\n",
    "           else:\n",
    "                ob16_csv.loc[i,'Tertile'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script is now our function. We can now return to outside of the function and thus return to the far left indent. We now need to 'call' the function to create tertiles and it is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tertile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then check that the function worked, whilst sorting the OBESEPERYR6 values. This bit of code does just that, whilst reassigning this sorted dataset to the ob16_csv variable, overwriting the variable we used above, as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob16_csv = ob16_csv.sort_values('OBESEPERYR6')\n",
    "ob16_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Join the 'tertiled' CSV to our shapefile - ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we've now our csv file ready for joining. We wanted to join our table to our shapefile, ready for mapping, just as we'd do in arcgis. And when using code, it's (fortunately) pretty simple in this case. First we need to read in the LAD shapefile, so we can join our table to it. Here we use the ```gpd.read_file``` method to read in the shapefile and its respective geometry and set it as the variable: ```boundaries```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set LAD shapefile path to variable boundaries\n",
    "boundaries = gpd.read_file('Data/England_lad_2011.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the shapefile loaded, we can join to it but first we need one more step of processing. To make our join easier, we need to reindex our CSV. If you look at your table above, you can see that one of the columns is a bold column. This is the index column, a bit like the row ID column you'd find in ArcGIS! The index column is the default column used to join data in Python. To make our join easier, we're going to reindex our CSV to the LA_Name column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex our csv to column LA_NAME\n",
    "ob16_csv = ob16_csv.set_index('LA_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now join the table to the shapefile on the LA_Name column, using the ```.join``` method. Note, we still need to state which column we want to use within the shapefile. Also, if you look at the shapefile in ArcGIS, the LA_Name column is actually NAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with ArcGIS, we join the table TO the shapefile, using the .join method as follows:\n",
    "OB16_shp = boundaries.join(ob16_csv, on='NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Export the joined data to a shapefile, ready for mapping. - ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to export our shapefile so we can use it for mapping! To do so, we use another GeoPandas method: ```.to_file``` and we state where we want to save the file, with what name and the format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export joined dataset to shapefile\n",
    "OB16_shp.to_file(data_path + 'Shapefiles/' + '2016_obesity_LA.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you check your Data/Obesity folder, you should now find that you have a new shapefile - and we've finished writing our code. We have script that does everything we want to get out data processed and ready for mapping..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three: Processing multiple CSVs at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or so we think. The problem is, we've still got to compute the tertiles for the other five files and then join them to the shapefile, time-consuming stuff! \n",
    "\n",
    "A simple way forward would be to repeat the process we've just been through by rewriting the code above using the appropriate file path to the csv and then changing the variable. It's certainly much quicker than doing the same thing in excel but (a) it can lead to mistakes and (b) we can make the process even faster by using a function. (There's also a (c), which is unless you save each iteration of code in a separate file, you're going to have to re-edit and re-edit and re-edit, if you make any other mistakes or changes).\n",
    "\n",
    "Instead, what we can do is rewrite our function so that it can do this for us. To do this, we need to make the function more general by using and passing arguments within the function itself rather than using specific variables and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Changing our code to be reusable, more useful and efficient using functions and arguments -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function can be used to repeat the same processes over and over again - we can just change what it iterates over, in our case the individual CSVs. To do this, we add arguments to our function that will be defined when we call the function - these are put in the parentheses of our function, separated by a comma. These arguments are the CSV and the column which we want the tertile function to use. All sounds a bit confusing?! Hopefully seeing the code in action will help you understand what is going on! \n",
    "\n",
    "We're going to create a function from the above `create_tertile` function, but it will determine the tertile on a csv which is denoted as ```mytable``` (rather than a precise variable) and a column denoted as ```scorevar``` i.e. score variable (instead of OBESEPERYR6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new function: determine_tertile, which takes the arguments: mytable,scorevar\n",
    "def determine_tertile(mytable,scorevar):\n",
    "    \"\"\"Creates and assigns a tertile value for a column of values\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    mytable: csv containing data\n",
    "    scorevar: column on which to run the tertile function\n",
    "\n",
    "    \"\"\"\n",
    "    mytable['Tertile'] = 0\n",
    "    tertile1 = np.percentile(mytable[scorevar], 33.33)\n",
    "    tertile2 = np.percentile(mytable[scorevar], 66.66)\n",
    "    print(tertile1, tertile2)\n",
    "\n",
    "    for i in range(len(mytable)):\n",
    "           if mytable.loc[i,scorevar] <= tertile1:\n",
    "                mytable.loc[i,'Tertile'] = 1\n",
    "           elif mytable.loc[i,scorevar] > tertile1 and mytable.loc[i,scorevar] <= tertile2:\n",
    "                mytable.loc[i,'Tertile'] = 2\n",
    "           elif mytable.loc[i,scorevar] > tertile2:\n",
    "                mytable.loc[i,'Tertile'] = 3\n",
    "           else:\n",
    "                pass\n",
    "                \n",
    "    return mytable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, it's nearly the same code as above, but we've replaced ```ob16_csv``` with ```mytable``` and ```OBESEPERYR6``` with ```scorevar``` within the function. When you call the function, you simply add the csv and column into the parenthesis. Then these are substituted automatically by the computer into the function, replacing each instance of ```mytable``` and ```scorevar``` where applicable. \n",
    "\n",
    "The function also returns the modified dataset as the same variable ```mytable```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Calling a function once but on multiple files -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our function ready to be used within our code, but we now need to ensure that we apply the function to each of our CSVs. So we need to figure out how to parse each of the CSVs and their respective column into the function... in one.\n",
    "\n",
    "To do so, we're going to use another ```for loop```. (You start to see how the same 'techniques' in Python can be used time and time again for multiple purposes).\n",
    "\n",
    "We're going to create a ```for loop``` that:\n",
    "\n",
    "* Loops through each of our CSVs in our Data/Obesity folder\n",
    "* Reads the data\n",
    "* Sets this data (i.e. the csv) to be equal to csv_path\n",
    "* Creates the tertile column and adds null values\n",
    "* Calls the ```determine_tertile``` function passing the ```csv_path``` in place of ```mytable``` and ```OBESEPERYR6``` in place of ```scorevar``` (have a look below).\n",
    "* Returns the tertiled csv to the variable ```csv_tertile```\n",
    "* Sets the index of the ```csv_tertile``` to ```LA_Name```\n",
    "* Loads the LA shapefile\n",
    "* Joins the shapefile and table together\n",
    "* Exports the data to a shapefile within your folder\n",
    "\n",
    "All in about 8 lines of code. In fact, it takes more words to explain the code than write it!\n",
    "\n",
    "So let's go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our AWESOME multi-tasking 'for loop':\n",
    "\n",
    "# For each year in the range 2011-2016, i.e. 2011, 2012, 2013, 2014, 2015.\n",
    "# Note a range in Python includes the first number but excludes the last i.e. 2016.\n",
    "for year in range(2011,2016):\n",
    "    # Set csv_path equal to the following file path\n",
    "    # Using the 'current' year in the loop to replace the year in the str(year)\n",
    "    # I.e. for 2013 loop, str(year) will be data_path + 2013 + '_LA_OBESITY.csv'\n",
    "    csv_path = data_path + str(year) + '_LA_OBESITY.csv'\n",
    "    # Read in the data at this path\n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "\n",
    "    # Create a variable called csv_tertile, which is equal to the output of calling (running) the determine_tertile function (above)\n",
    "    # on the variable csv_data and it's column 'OBESEPERYR6'\n",
    "    csv_tertile = determine_tertile(csv_data, 'OBESEPERYR6')\n",
    "    \n",
    "    # Reindex this CSV using LA_NAME and then reassign this to the varaible csv_tertile \n",
    "    csv_tertile = csv_tertile.set_index('LA_NAME')\n",
    "    \n",
    "    # Load the LAD shapefile\n",
    "    boundaries = gpd.read_file('Data/England_lad_2011.shp')\n",
    "   \n",
    "    # Join the csv_tertile variable to the boundaries shapefile\n",
    "    map_shape = boundaries.join(csv_tertile, on='NAME')\n",
    "    \n",
    "    # Export the joined data to a shapefile ready for mapping\n",
    "    map_shape.to_file(data_path + 'Shapefiles/'+ str(year) +'_obesity_LA.shp', driver='ESRI Shapefile')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our entire 'For Loop' carries out each of the processing steps we went through earlier with the 2016 csv, for the remaining CSVs (2011-2015) from start (cleaned CSV) to finish (shapefile output). \n",
    "\n",
    "Hopefully you can see where the ```for loop``` calls on our pre-defined ```determine_tertile``` function to create the tertiles for the datasets. If you can't, feel free to ask for an explanation - and we'll also have a quick discussion at the end of the class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four: Mapping our shapefile outputs using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We finally have our shapefiles exported to folder, ready for mapping. We could use the shapefiles within ArcGIS and produce individual maps, export them as an ```svg``` to be used in a vector-based design software, such as 'Illustrator' or 'Graphic', or in our case, we're going to map the data within Python and then export our maps to create a dynamic GIF.\n",
    "\n",
    "To do so, we'll be using a ```for loop``` to repeat the same steps for each shapefile:\n",
    "* Read in the shapefile for that year\n",
    "* Set up a figure i.e. a graph in which the data will be mapped\n",
    "* Draw a choropleth map using the ```Tertile``` column, with a blue colour scheme.\n",
    "* Add a title, simply the year of each data, to the map\n",
    "* Remove the graph axis to have a clean aesthetic and to make it look more like a simple map\n",
    "* Ensure the data is presented proportionately within the map\n",
    "* Save the final map as a PNG to the ```Maps``` folder\n",
    "* Close the figure - this removes the figure from the computer memory, meaning that our processing runs quicker and smoother\n",
    "\n",
    "\n",
    "We can also use each exported PNG within a report or publication. As you'll see, its layout and formatting isn't brilliant at the moment, but a little time spent editing the code could help sharpen the map up.\n",
    "\n",
    "Let's make our GIF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to create a shortcut in one of our libraries so we don't have to type out the whole phrase\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we create our code - this time our range is extended to 2017 so we include our previously processed ```2011_LA_Obesity.shp```. I won't go into too much detail on what all the code means but feel free to ask if you'd like parts of it explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2011,2017):\n",
    "    map_data = gpd.read_file(data_path + 'Shapefiles/'+ str(year)+'_obesity_LA.shp', driver='ESRI Shapefile')\n",
    "   \n",
    "    #set up the figure\n",
    "    f, ax = plt.subplots(1, figsize=(3,4))\n",
    "    #set background color of the axis\n",
    "    ax.set_facecolor('w')\n",
    "\n",
    "    #draw the choropleth\n",
    "    map_data.plot(column='Tertile', categorical=True, cmap='Blues', \n",
    "               linewidth=0.2, edgecolor='gray',legend=True, ax=ax)\n",
    "    #add title\n",
    "    f.suptitle(str(year))\n",
    "\n",
    "    #remove axis\n",
    "    ax.set_axis_off()\n",
    "    #Keep axes proportionate\n",
    "    plt.axis('equal')\n",
    " \n",
    "    f.savefig('Maps/' + str(year) + '_LA_obesity_map.png', bbox_inches='tight')   # save the figure to file\n",
    "    plt.close(f)    # close the figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you find that your code breaks, particularly with the error ```nan```, it is because you have not cleaned your 2016 data appropriately and the join hasn't worked. Have a look at the shapefile in ArcGIS to see that the shapefile is empty. You need to re-open the csv in Excel and check your name matches - for example, both St Albans and St Edmondbury must be written without a . ; in contrast, St. Helens should contain a . . Once you've saved your csv again, you'll need to restart the Kernel, run the code again for the 2016 csv and then run the code just above to create the PNGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Creating a GIF from our images -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last thing for today and reaping the benefits of your hard work - with a tiny bit of code, we can have a little fun with our PNGs and make the start of what could be a really cool data visualisation.\n",
    "\n",
    "We're going to create a GIF that flicks through our six PNGs at one second at a time. Once it's generated, you need to open it within a interent browser to see the GIF. Just right-click on the GIF when it appears in your folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're importing a new library here: imageio . Yes, it helps us make GIFs.\n",
    "import imageio\n",
    "\n",
    "# We create an empty list to put each of our file names into.\n",
    "images = []\n",
    "# Use a for loop (again!)\n",
    "# For each year in our range:\n",
    "for year in range(2011, 2017):\n",
    "    # Set a variable called image_path, equal to the year PNG\n",
    "    image_path = 'Maps/' + str(year) + '_LA_obesity_map.PNG'\n",
    "    # Read in the image (using the imageio.imread method) and set it to image_data\n",
    "    image_data = imageio.imread(image_path)\n",
    "    \n",
    "    # Add this image to our image list \n",
    "    images.append(image_data)\n",
    "\n",
    "# Once the loop has finished adding each PNG to the list\n",
    "# Use the imageio.mimsave method to create a GIF called obesity, using the images found in the images list\n",
    "# And set the duration to 1 second - you can change this if you want!\n",
    "imageio.mimsave('Maps/obesity.GIF', images, duration=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You should now be able to find your GIF waiting for you in your Maps folder. Open up and enjoy! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
